General bioinformatics workflow
================
James C. Kosmopoulos
2023-12-05

# READ ME

The following is the general workflow used to for the virome
vs. metagenome analysis. Basic commands and scripts are given that were
applied to the four tested environments, separately. Some commands have
variable arguments that will need to be updated for the user that is
reproducing this analysis.

# Read QC and processing

## Interleave reads with BBTools reformat.sh

``` bash
# Requires BBTools suite v38.86
MEM="Xmx300g"
for F in raw_reads/*_1.fastq; do
    R=${F%_*}_2.fastq
    BASE=${F##*/}
    SAMPLE=${BASE%_*}
    OUT="/raw_reads/$SAMPLE.fastq"
    reformat.sh -$MEM in=$F in2=$R out=$OUT
    gzip $OUT
    rm $F
    rm $R
done
```

## Get read length statistics with BBTools readlen.sh

``` bash
# Requires BBTools suite v38.86
MEM="Xmx300g"
for F in raw_reads/*.fastq.gz; do
    echo $F
    BASE=${F##*/}
    SAMPLE=${BASE%?????????}
  OUT="/processed_reads/"$SAMPLE"_readlen.txt"
    readlength.sh -$MEM in=$F out=$OUT overwrite
done
```

## Run BBTools rqcfilter2.sh on interleaved reads

``` bash
# Requires BBTools suite v38.86
DATADIR="RQCFilterData"
MEM="Xmx100g"
THREADS="16"
for F in raw_reads/*.fastq.gz; do
        BASE=${F##*/}
        SAMPLE=${BASE%?????????}
        OUT="/processed_reads/"$SAMPLE"_filtered.fastq.gz"
        rqcfilter2.sh -$MEM chastityfilter=f jni=t in=$F rqcfilterdata=$DATADIR \
        path=filter rna=f trimfragadapter=t qtrim=r trimq=0 maxns=3 maq=3 minlen=51 \
        mlf=0.33 phix=t removehuman=t removedog=t removecat=t removemouse=t khist=t \
        removemicrobes=f sketch kapa=t clumpify=t tmpdir= barcodefilter=f trimpolyg=5 usejni=f \
        out=$OUT threads=$THREADS
done
```

## Read error correction with BBTools bbcms.sh

``` bash
# Requires BBTools suite v38.86
MEM="Xmx100g"
THREADS="16"
for F in processed_reads/*_filtered.fastq.gz; do
    BASE=${F##*/}
    SAMPLE=${BASE%??????????????????}
    OUT="corrected_reads/"$SAMPLE"_corrected.fastq.gz"
    bbcms.sh -$MEM t=$THREADS mincount=2 highcountfraction=0.6 in=$F out=$OUT
done
```

## Separate paired and unpaired reads with BBTools repair.sh

``` bash
# Requires BBTools suite v38.86
MEM="Xmx100g"
for F in corrected_reads/*_corrected.fastq.gz; do
    BASE=${F##*/}
    SAMPLE=${BASE%???????????????????}
    OUT="repaired_reads/"$SAMPLE"_repaired.fastq.gz"
  OUTS="repaired_reads/"$SAMPLE"_singletons.fastq.gz"
    repair.sh -$MEM in=$F out=$OUT outs=$OUTS repair
done
```

# Assemble error-corrected reads with SPAdes

## Run SPAdes

Run for every set of error-corrected, interleaved, paired and unpaired
reads. Command for one sample is given as an example.

``` bash
# Requires metaSPAdes v3.13.0
spades.py -m 512 --tmp-dir spades_tmp -o spades_assemblies/SRR8487010_spades_out \
--only-assembler -k 33,55,77,99,127 --meta -t 30 \
--12 repaired_reads/SRR8487010_repaired.fastq.gz -s repaired_reads/SRR8487010_singletons.fastq.gz
```

## Move assembled contigs into a new folder

``` bash
SRC_DIR="spades_assemblies"
DEST_DIR="assembly_fastas"

for folder in ${SRC_DIR}/*_spades_out; do
     SAMPLE_NAME=$(basename $folder | cut -d'_' -f1)
     SRC_FILE="${folder}/contigs.fasta"
     DEST_FILE="${DEST_DIR}/${SAMPLE_NAME}_contigs.fna"
     cp $SRC_FILE $DEST_FILE
done
```

## Get assembly reports from metaQUAST

``` bash
# Requires metaQUAST v5.2.0 
metaquast assembly_fastas/*_contigs.fna \
-m 2000 --contig-thresholds 0,2000,5000,10000,25000,50000 \
--max-ref-number 0 --fast -s -o metaquast_reports
```

# Run ViWrap

## Separate read mates for ViWrap

``` bash
# Requires BBTools suite v38.86
MEM="Xmx100g"
MAX_JOBS=11
execute_repair() {
    local F="$1"
    local BASE="${F%_repaired.fastq.gz}"
    local SAMPLE="${BASE##*/}"
    local OUT="$2/"$SAMPLE"_1.fastq.gz"
    local OUT2="$2/"$SAMPLE"_2.fastq.gz"
    bash repair.sh -$MEM in=$F out=$OUT out2=$OUT2 repair overwrite
}

count=0
for F in repaired_reads/*_repaired.fastq.gz; do
    execute_repair "$F" "repaired_reads" &
    ((count++))
    if (( count % MAX_JOBS == 0 )); then
        wait
    fi
done
wait
```

## Execute ViWrap

Run for every assembly along with its corresponding sets of paired
reads. Command for one sample is given as an example.

``` bash
# Requires ViWrap v1.2.1
ViWrap run --input_metagenome assembly_fastas/SRR8487010_contigs.fna \
--input_reads repaired_reads/SRR8487010_R1.fastq.gz,repaired_reads/SRR8487010_R2.fastq.gz \
--out_dir ViWrap_out/SRR8487010_ViWrap_out \
--db_dir ViWrap_db --threads 10 \
--identify_method vb \
--conda_env_dir ViWrap_conda_environments \
--input_length_limit 10000 --reads_mapping_identity_cutoff 0.90
```

## Get virus stats from ViWrap

### First need to get a path of all summary stat files generated by ViWrap

``` bash
find ViWrap_out -name "virus_statistics.txt" > viwrap_virus_stats_paths.txt
```

### Now parse and combine results using python

``` python3
import os
import pandas as pd

def VirStats2df(pathfile):
    # Subset existing dataframe to get table of samples
    data_frame_pre = data_frame_plus[["sample_source", "Environment", "Method"]]
    data_frame_pre.index.name = None

    with open(pathfile) as f:
        paths = [line.rstrip() for line in f]

    df_list = list()

    for path in paths:
        sample = path.replace("_ViWrap_out/09_Virus_statistics_visualization/Result_visualization_inputs/virus_statistics.txt", "")
        samples = sample.split("/")
        sample = (samples[-1:])[0]
        df =  pd.read_csv(path, sep="\t")
        df.insert(0, "Sample", sample)
        df = df.set_index(["Sample"])
        df.index.name = None
        df_list.append(df)
    
    data_frame = pd.concat(df_list)
    data_frame2 = pd.concat([data_frame_pre, data_frame], axis = 1)
    data_frame2 = data_frame2.apply(pd.to_numeric, errors='ignore')
    data_frame2.columns = data_frame2.columns.str.replace("[.]", "")
    return data_frame2

data_frame_virstats = VirStats2df("viwrap_virus_stats_paths.txt")
data_frame_virstats.to_csv("Tables/virus_stats.tsv", sep="\t")
```

## Get additional virus-level info from ViWrap

Such as predicted lytic state (will combine “scaffold” and “virus”
distinctions), CheckV quality, and taxonomy

``` python3
import os
import pandas as pd

parent_dir = "ViWrap_out/"
outdir_vir = "Tables/"
outdir_tax = "Tables/"

df_virus_list = list()
df_tax_list = list()

# Get paths to needed files containing virus data:
for root, dirs, files in os.walk(parent_dir): # Walk through contents of ViWrap results directory
    if "08_ViWrap_summary_outdir" in root:
        for file in files:
            if "Virus_summary_info.txt" in file: # We want this file
                filepath = root + "/" + file
                sample = (filepath.split("/")[-3]).replace("_ViWrap_out", "")
                newname = f"{sample}_Virus_summary_info.txt"
                df_virus = pd.read_csv(filepath, sep="\t", index_col=False)
                df_virus = df_virus.rename(columns = {"Unnamed: 0":"genome"})
                df_virus = df_virus.replace("vRhyme_",f"{sample}__vRhyme_", regex=True) # Add sample prefix to VMAG name
                df_virus["sample"] = [sample] * len(df_virus.index)
                df_virus = df_virus.set_index("sample", drop=True)
                df_virus_list.append(df_virus)
            if "Tax_classification_result.txt" in file: # We want this file
                filepath = root + "/" + file
                sample = (filepath.split("/")[-3]).replace("_ViWrap_out", "")
                newname = f"{sample}_Tax_classification_result.txt"
                df_tax = pd.read_csv(filepath, index_col=False, names=["genome", "taxonomy", "source"], sep = "\t")
                df_tax = df_tax.replace("vRhyme_",f"{sample}__vRhyme_", regex=True)
                df_tax["sample"] = [sample] * len(df_tax.index)
                df_tax = df_tax.set_index("sample", drop=True)
                df_tax_list.append(df_tax)

df_virus_concat = pd.concat(df_virus_list, axis = 0)
df_virus_concat.to_csv(f"{outdir_vir}virus_summary_info_combined.tsv", sep = "\t")
df_tax_concat = pd.concat(df_tax_list, axis = 0)
df_tax_concat.to_csv(f"{outdir_tax}virus_tax_classification_results_combined.tsv", sep = "\t")
```

## Get read recruitment from ViWrap

### For only viral contigs identified by VIBRANT

Also assuming input length requirement for ViWrap/VIBRANT was 10 kb.

Requires SAMtools v1.17

``` python3
import multiprocessing
import subprocess
import os
import pandas as pd

parent = "ViWrap_out/"

contig_dict = {}
for root, dirs, files in os.walk(parent):
    for file in files:
        if file.endswith(".phages_combined.fna"):
            filepath = os.path.join(root, file)
            sample = filepath.split("/")[-4]
            if "_contigs" in sample:
               sample = sample.replace("_contigs_ViWrap_out", "")
            else:
               sample = sample.replace("_ViWrap_out", "")
            contig_dict[sample] = filepath

def get_bam_paths():
    bam_paths = []
    samples = []
    for root, dirs, files in os.walk(parent):
        for file in files:
            if file.endswith(".bam"):
                filepath = os.path.join(root, file)
                sample = filepath.split("/")[-3]
                if "_contigs" in sample:
                   sample = sample.replace("_contigs_ViWrap_out", "")
                else:
                   sample = sample.replace("_ViWrap_out", "")
                bam_paths.append(filepath)
                samples.append(sample)
    return bam_paths, samples

def get_contig_list(sample):
    contig_list = []
    with open(contig_dict[sample]) as f:
        lines = f.readlines()
        for line in lines:
            if line.startswith(">"):
              line = line.strip()
              line = line.replace(">","")
              contig_list.append(line)
    return contig_list

def get_sum_of_uniquely_mapped_reads(bam_file, sample, contigs_of_interest):
  """
  Gets the sum of all read pairs that uniquely mapped to the contigs of interest.

  Args:
    bam_file: The path to the BAM file.
    contigs_of_interest: A list of the contigs of interest.

  Returns:
    The sum of all read pairs that uniquely mapped to the contigs of interest.
  """

  # Extract the read pairs that uniquely mapped to the contigs of interest.
  samtools_view_command = f"samtools view -F 260 -b {bam_file} | samtools sort -o {sample}_unique.bam"
  samtools_index_command = f"samtools index {sample}_unique.bam -o {sample}_unique.bam.bai"
  if os.path.isfile(f"{sample}_unique.bam"):
      if os.path.isfile(f"{sample}_unique.bam.bai") == False:
        subprocess.call(samtools_index_command, shell=True)
  else:
     subprocess.call(samtools_view_command, shell=True)
     subprocess.call(samtools_index_command, shell=True)
        
  # Count the number of read pairs that uniquely mapped to the contigs of interest.
  samtools_idxstats_command = f"samtools idxstats {sample}_unique.bam"
  results = subprocess.run(samtools_idxstats_command, shell=True, text=True, capture_output=True).stdout

  # Get the sum of the number of read pairs that uniquely mapped to the contigs of interest.
  sum_of_reads = 0
  for line in results.splitlines():
    contig, length, reads, unmapped = line.split()
    if contig in contigs_of_interest:
      sum_of_reads += int(reads)

  return sample, sum_of_reads

def main():
  results = get_bam_paths()
  bam_files = results[0]
  samples = results[1]
  contigs = []
  for sample in samples:
    contigs.append(get_contig_list(sample))

  # Create a pool of 42 processes.
  pool = multiprocessing.Pool(42)
     
  # Map the function to each BAM file in the list.
  pool_out = pool.starmap(get_sum_of_uniquely_mapped_reads, zip(bam_files, samples, contigs))
  df_list = []
  for result in pool_out:
    sample = result[0]
    readsum = result[1]
    print(sample, readsum)
    df = pd.DataFrame({"Sample" : [sample], "sum_paired_reads_mapped_to_contigs" : [readsum]})
    df_list.append(df)
  df = pd.concat(df_list)
  df.to_csv("Tables/read_mapping_stats_sum.tsv", sep="\t", index=False)

if __name__ == "__main__":
  main()
```

### For all input contigs \>10 kb (viral and non-viral)

Requires SAMtools v1.17

``` python3
import pandas as pd
import subprocess
import os

parent = "ViWrap_out/"

df_list = []
for root, dirs, files in os.walk(parent):
    for file in files:
        if file.endswith(".bam"):
            filepath = os.path.join(root, file)
            sample = filepath.split("/")[-3]
            if "_contigs" in sample:
               sample = sample.replace("_contigs_ViWrap_out", "")
            else:
               sample = sample.replace("_ViWrap_out", "")
            cmd = f"samtools idxstats {filepath}"
            results = subprocess.run(cmd, shell=True, text=True, capture_output=True).stdout
            read_pairs = 0
            sort_length = True
            while sort_length == True:
                for line in results.splitlines():
                    contig, length, reads, unmapped = line.split()
                    reads = int(reads)
                    length = int(length)
                    if length > 9999: # Set to minimum contig length
                        read_pairs += reads
                    else:
                        sort_length = False
            df = pd.DataFrame({"Sample" : [sample], "sum_paired_reads_mapped_to_contigs_10000" : [read_pairs]})
            df_list.append(df)
df = pd.concat(df_list)
df.to_csv("Tables/read_mapping_stats_contigs10000_sum.tsv", sep="\t", index=False)
```

# Read mapping to viral genomes

For presence/absence analysis

## Copy vMAGs from ViWrap results

Assumes ViWrap output folder was organized into subfolders for each
environment (human gut, freshwater, marine, soil) and extraction method
(virome, metagenome). This was not shown above.

``` python3
import os
import shutil

parent_path = "ViWrap_out/"
out_dir = "virus_genomes/individual"

species_counts_dict = dict()
total_species_info_lines = 0
vmag_counts = 0

cluster_list_dict = dict()
for root, dirs, files in os.walk(parent_path): 
    if "Virus_genomes_files" in root:
        sample = root.split("/")[-3]
        sample = sample.replace("_contigs_ViWrap_out", "")
        for file in files:
            if file.endswith(".fasta"):
                fasta = f"{root}/{file}"
                newfilename = f"{sample}__{file}"
                if "human_gut" in root:
                    env = "human_gut"
                elif "freshwater" in root:
                    env = "freshwater"
                elif "marine" in root:
                    env = "marine"
                elif "soil" in root:
                    env = "soil"
                outpath = f"{out_dir}/{env}/{newfilename}"
                with open(outpath, "wb") as fdst:
                    with open(fasta, "rb") as fsrc:
                        shutil.copyfileobj(fsrc, fdst)
```

## Combine fastas from the same environment into one, each

``` bash
find virus_genomes/individual/human_gut/ -name "*.fasta" -print0 | xargs -0 cat > virus_genomes/human_gut_vmags.fasta

find virus_genomes/individual/freshwater/ -name "*.fasta" -print0 | xargs -0 cat > virus_genomes/freshwater_vmags.fasta

find virus_genomes/individual/marine/ -name "*.fasta" -print0 | xargs -0 cat > virus_genomes/marine_vmags.fasta

find virus_genomes/individual/soil/ -name "*.fasta" -print0 | xargs -0 cat > virus_genomes/soil_vmags.fasta
```

## Get a list of the paths to all of these VMAGs, to be dereplicated, below

``` bash
find virus_genomes/individual/human_gut -name "*.fasta" > virus_genomes/individual/human_gut_indiv_redundant_vmags_list.txt

find virus_genomes/individual/freshwater -name "*.fasta" > virus_genomes/individual/freshwater_indiv_redundant_vmags_list.txt

find virus_genomes/individual/marine -name "*.fasta" > virus_genomes/individual/marine_indiv_redundant_vmags_list.txt

find virus_genomes/individual/soil -name "*.fasta" > virus_genomes/individual/soil_indiv_redundant_vmags_list.txt
```

## Dereplicate vMAGs in each environment into species-level clusters

Run dRep for each environment. Requires dRep v3.4.3.

``` bash
dRep dereplicate -g virus_genomes/individual/human_gut_indiv_redundant_vmags_list.txt \
--ignoreGenomeQuality -pa 0.8 -sa 0.95 -nc 0.85 -comW 0 -conW 0 -strW 0 -N50W 0 -sizeW 1 -centW 0 \
-l 10000 --skip_plots -p 30 dRep_out/human_gut

dRep dereplicate -g virus_genomes/individual/freshwater_indiv_redundant_vmags_list.txt \
--ignoreGenomeQuality -pa 0.8 -sa 0.95 -nc 0.85 -comW 0 -conW 0 -strW 0 -N50W 0 -sizeW 1 -centW 0 \
-l 10000 --skip_plots -p 30 dRep_out/freshwater

dRep dereplicate -g virus_genomes/individual/marine_indiv_redundant_vmags_list.txt \
--ignoreGenomeQuality -pa 0.8 -sa 0.95 -nc 0.85 -comW 0 -conW 0 -strW 0 -N50W 0 -sizeW 1 -centW 0 \
-l 10000 --skip_plots -p 30 dRep_out/marine

dRep dereplicate -g virus_genomes/individual/soil_indiv_redundant_vmags_list.txt \
--ignoreGenomeQuality -pa 0.8 -sa 0.95 -nc 0.85 -comW 0 -conW 0 -strW 0 -N50W 0 -sizeW 1 -centW 0 \
-l 10000 --skip_plots -p 30 dRep_out/soil
```

## Gather dereplicated vMAGs and combine into one fasta

``` bash
find dRep_out/human_gut/dereplicated_genomes/ -name "*.fasta" -print0 | xargs -0 -I {} cp {} virus_species_fastas/human_gut/
find virus_species_fastas/human_gut/ -name "*.fasta" -print0 | xargs -0 cat > virus_species_fastas/human_gut_species_vmags.fasta

find dRep_out/freshwater/dereplicated_genomes/ -name "*.fasta" -print0 | xargs -0 -I {} cp {} virus_species_fastas/freshwater/
find virus_species_fastas/freshwater/ -name "*.fasta" -print0 | xargs -0 cat > virus_species_fastas/freshwater_species_vmags.fasta

find dRep_out/marine/dereplicated_genomes/ -name "*.fasta" -print0 | xargs -0 -I {} cp {} virus_species_fastas/marine/
find virus_species_fastas/marine/ -name "*.fasta" -print0 | xargs -0 cat > virus_species_fastas/marine_species_vmags.fasta

find dRep_out/soil/dereplicated_genomes/ -name "*.fasta" -print0 | xargs -0 -I {} cp {} virus_species_fastas/soil/
find virus_species_fastas/soil/ -name "*.fasta" -print0 | xargs -0 cat > virus_species_fastas/soil_species_vmags.fasta
```

## Build a Bowtie2 mapping index for each combined fasta

Run for each environment. Requires Bowtie2 v2.5.1.

``` bash
bowtie2-build --threads 30 \
virus_species_fastas/human_gut_species_vmags.fasta \
map_species_vmags/indexes/human_gut/index

bowtie2-build --threads 30 \
virus_species_fastas/freshwater_species_vmags.fasta \
map_species_vmags/indexes/freshwater/index

bowtie2-build --threads 30 \
virus_species_fastas/marine_species_vmags.fasta \
map_species_vmags/indexes/marine/index

bowtie2-build --threads 30 \
virus_species_fastas/soil_species_vmags.fasta \
map_species_vmags/indexes/soil/index
```

## Map reads to indices with Bowtie2

Run for each set of paired reads for each environment. Command for one
sample is given as an example. Requires Bowtie2 v2.5.1.

``` bash
bowtie2 --end-to-end --sensitive -p 30 \
-x map_species_vmags/indexes/soil/index --mm \
-1 repaired_reads/SRR8487010_R1.fastq.gz -2 repaired_reads/SRR8487010_R2.fastq.gz \
-S map_species_vmags/mapfiles/soil/SRR8487010.sam
```

## Convert sam to sorted bam

Using SAMtools v1.17

``` bash
ls map_species_vmags/mapfiles/human_gut/*.sam | parallel --jobs 42 'samtools view -bS {} | samtools sort -o map_species_vmags/mapfiles/human_gut/{.}.bam -'

ls map_species_vmags/mapfiles/freshwater/*.sam | parallel --jobs 42 'samtools view -bS {} | samtools sort -o map_species_vmags/mapfiles/freshwater/{.}.bam -'

ls map_species_vmags/mapfiles/marine/*.sam | parallel --jobs 42 'samtools view -bS {} | samtools sort -o map_species_vmags/mapfiles/marine/{.}.bam -'

ls map_species_vmags/mapfiles/soil/*.sam | parallel --jobs 42 'samtools view -bS {} | samtools sort -o map_species_vmags/mapfiles/soil/{.}.bam -'
```

## Index sorted bam files

``` bash
ls *.bam | parallel --jobs 42 'samtools index {}' & # Execute in mapfile dir for each environment
```

## Filter reads mapping \>= 90% identity with CoverM

Requires CoverM v0.6.1

``` bash
# Execute in mapfile dir for each environment
for BAM in *.bam; do NAME=${BAM%.*}
  coverm filter -b $BAM -o $NAME.filtered.bam --min-read-percent-identity 90 --threads 10
done

ls *.filtered.bam | parallel --jobs 42 'samtools index {}' & # indexes filtered bam files, too
```

## Generate covered fraction (genome breadth) tables with CoverM

Here I am setting the minimum covered fraction to 0 so I can obtain all
values even if they are below 0.75. I will filter values \< 0.75 later
on. Requires CoverM v0.6.1.

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
for BAM in map_species_vmags/mapfiles/marine/*.filtered.bam; do
  BASE=$(basename $BAM)
  NAME=${BASE%%.*}
  coverm genome -b $BAM -d virus_species_fastas/marine/ \
  -x fasta -m covered_fraction --min-covered-fraction 0 \
  -o vmag_coverage/tara/$NAME.covered_fraction.cov -t 10
done
```

## Combine genome breadth tables into matrices for each environment

``` python3
import pandas as pd
import glob

parent = "vmag_coverage/"
envs = ["human_gut", "freshwater", "marine", "soil"]
outparent = "Tables/"

for env in envs:
    path = parent + env
    df_fractions_list = []
    for file in glob.glob(path + "/*"):
        df = pd.read_csv(file, sep = "\t", index_col=0)
        df.index.name = None
        df.columns = [df.columns[0].split(".")[0]]
        if file.endswith(".covered_fraction.cov"):
            df_fractions_list.append(df)
    df_fractions = pd.concat(df_fractions_list, axis = 1)
    df_fractions.to_csv(f"{outparent}covered_fraction_{env}.tsv", sep = "\t")
```

# Identify candidate genomes assembled from viromes and metagenomes to compare

I did this iteratively by manually searching through the summary info
tables for the species-representative vMAGs (in decreasing order of
genome length) until I found a vMAG that satisfied the criteria laid out
in the methods of the paper:

- One vMAG was assembled from a virome and the other a metagenome
- Each vMAG in the pair was placed in the same species-level cluster
- Both vMAGs were assembled from the same sample source
- The virome-assembled vMAG was a single contig and predicted by CheckV
  to be complete
- The metagenome-assembled vMAG was predicted by CheckV to be incomplete

This ended up being the vMAG named Ga0485184\_\_vRhyme_unbinned_566. So,
I used this in the example below.

## First, get species cluster membership information from dRep

``` python3
import csv

def find_secondary_cluster(input_file_path, query):
    # Read the input file
    with open(input_file_path, 'r') as file:
        reader = csv.DictReader(file)
        data = list(reader)

    # Extract genomes and secondary clusters
    genomes_secondary_clusters = [(row['genome'].replace(".fasta",""), row['secondary_cluster']) for row in data]

    # Find the secondary cluster for the queried genome
    for genome, secondary_cluster in genomes_secondary_clusters:
        if genome == query:
            return secondary_cluster, [g for g, s in genomes_secondary_clusters if s == secondary_cluster]

    return None, None

freshwater_members = find_secondary_cluster("/storage2/scratch/kosmopoulos/virome_vs_metagenome/dRep_out/freshwater/data_tables/Cdb.csv", "Ga0485184__vRhyme_unbinned_566")
print(freshwater_members)
```

This resulted in the following:
`('4649_3', ['Ga0485171__vRhyme_unbinned_192', 'Ga0485172__vRhyme_unbinned_38', 'Ga0485184__vRhyme_unbinned_566'])`.
Of these, the vMAG that is from the same sample source (to satisfy the
third criterion above) is Ga0485172\_\_vRhyme_unbinned_38.

## Map reads to the two vMAGs

### Create Bowtie2 indices for each vMAG

Requires Bowtie2 v2.5.1

``` bash
bowtie2-build --threads 30 \
virus_genomes/individual/freshwater/Ga0485184__vRhyme_unbinned_566.fasta \
map_species_vmags/indexes/individual/Ga0485184__vRhyme_unbinned_566

bowtie2-build --threads 30 \
virus_genomes/individual/freshwater/Ga0485172__vRhyme_unbinned_38.fasta \
map_species_vmags/indexes/individual/Ga0485172__vRhyme_unbinned_38
```

### Map reads to vMAG indices

Map reads from each sample source to each vMAG, so four Bowtie2 runs.
Requires Bowtie2 v2.5.1.

``` bash
bowtie2 --end-to-end -p 30 \
-x map_species_vmags/indexes/individual/Ga0485172__vRhyme_unbinned_38 --mm \
-1 repaired_reads/Ga0485172_R1.fastq.gz -2 repaired_reads/Ga0485172_R2.fastq.gz \
-S map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485172__vRhyme_unbinned_38.sam

bowtie2 --end-to-end -p 30 \
-x map_species_vmags/indexes/individual/Ga0485184__vRhyme_unbinned_566 --mm \
-1 repaired_reads/Ga0485172_R1.fastq.gz -2 repaired_reads/Ga0485172_R2.fastq.gz \
-S map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485184__vRhyme_unbinned_566.sam

bowtie2 --end-to-end -p 30 \
-x map_species_vmags/indexes/individual/Ga0485172__vRhyme_unbinned_38 --mm \
-1 repaired_reads/Ga0485184_R1.fastq.gz -2 repaired_reads/Ga0485184_R2.fastq.gz \
-S map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485172__vRhyme_unbinned_38.sam

bowtie2 --end-to-end -p 30 \
-x map_species_vmags/indexes/individual/Ga0485184__vRhyme_unbinned_566 --mm \
-1 repaired_reads/Ga0485184_R1.fastq.gz -2 repaired_reads/Ga0485184_R2.fastq.gz \
-S map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485184__vRhyme_unbinned_566.sam
```

### Convert sam to sorted bam

Using SAMtools v1.17

``` bash
samtools view -bS map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485172__vRhyme_unbinned_38.sam | samtools sort -o map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485172__vRhyme_unbinned_38.bam

samtools view -bS map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485184__vRhyme_unbinned_566.sam | samtools sort -o map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485184__vRhyme_unbinned_566.bam

samtools view -bS map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485172__vRhyme_unbinned_38.sam | samtools sort -o map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485172__vRhyme_unbinned_38.bam

samtools view -bS map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485184__vRhyme_unbinned_566.sam | samtools sort -o map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485184__vRhyme_unbinned_566.bam

ls map_species_vmags/mapfiles/individual/*.bam | parallel --jobs 4 'samtools index {}'
```

### Get read depths per base for each mapfile

Using SAMtools v1.17

``` bash
samtools depth map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485172__vRhyme_unbinned_38.bam > vmag_coverage/Ga0485172_to_Ga0485172__vRhyme_unbinned_38.depth_per_base.cov

samtools depth map_species_vmags/mapfiles/individual/Ga0485172_to_Ga0485184__vRhyme_unbinned_566.bam > vmag_coverage/Ga0485172_to_Ga0485184__vRhyme_unbinned_566.depth_per_base.cov

samtools depth map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485172__vRhyme_unbinned_38.bam > vmag_coverage/Ga0485184_to_Ga0485172__vRhyme_unbinned_38.depth_per_base.cov

samtools depth map_species_vmags/mapfiles/individual/Ga0485184_to_Ga0485184__vRhyme_unbinned_566.bam > vmag_coverage/Ga0485184_to_Ga0485184__vRhyme_unbinned_566.depth_per_base.cov
```

The four resulting files were manually combined into one table with an
added column specifying the read sample.

## Run Pharokka on each vMAG to obtain genome annotations

Requires Pharokka v1.4.1

``` bash
pharokka.py -i virus_genomes/individual/freshwater/Ga0485184__vRhyme_unbinned_566.fasta \
-o pharokka_out/indiv_genomes/Ga0485184__vRhyme_unbinned_566 \
-p Ga0485184__vRhyme_unbinned_566 \
-d pharokka_db -t 30 -g phanotate --dnaapler

pharokka.py -i virus_genomes/individual/freshwater/Ga0485172__vRhyme_unbinned_38.fasta \
-o pharokka_out/indiv_genomes/Ga0485172__vRhyme_unbinned_38 \
-p Ga0485172__vRhyme_unbinned_38 \
-d pharokka_db -t 30 -g phanotate --dnaapler
```

# Read mapping to viral genes

For differential abundance analysis.

## Gather protein-coding genes identified on viral contigs

``` python3
import os
import shutil

parent_path = "ViWrap_out"
out_dir = "virus_genes"

all_genes_gut = ""
all_genes_fw = ""
all_genes_mar = ""
all_genes_soil = ""

cluster_list_dict = dict()
for root, dirs, files in os.walk(parent_path): 
    if "Virus_genomes_files" in root:
        sample = root.split("/")[-3]
        sample = sample.replace("_ViWrap_out", "")
        for file in files:
            if file.endswith(".ffn"):
                fasta = f"{root}/{file}"
                newfilename = f"{sample}__{file}"
                if "human_gut" in root:
                    env = "human_gut"
                    outpath = f"{out_dir}/individual/{env}/{newfilename}"
                    with open(outpath, "wb") as fdst:
                        with open(fasta, "rb") as fsrc:
                            shutil.copyfileobj(fsrc, fdst)
                    with open(fasta, "r") as fsrc:
                        lines = fsrc.readlines()
                    for line in lines:
                        all_genes_gut += line
                elif "freshwater" in root:
                    env = "freshwater"
                    outpath = f"{out_dir}/individual/{env}/{newfilename}"
                    with open(outpath, "wb") as fdst:
                        with open(fasta, "rb") as fsrc:
                            shutil.copyfileobj(fsrc, fdst)
                    with open(fasta, "r") as fsrc:
                        lines = fsrc.readlines()
                    for line in lines:
                        all_genes_fw += line
                elif "marine" in root:
                    env = "marine"
                    outpath = f"{out_dir}/individual/{env}/{newfilename}"
                    with open(outpath, "wb") as fdst:
                        with open(fasta, "rb") as fsrc:
                            shutil.copyfileobj(fsrc, fdst)
                    with open(fasta, "r") as fsrc:
                        lines = fsrc.readlines()
                    for line in lines:
                        all_genes_mar += line
                elif "soil" in root:
                    env = "soil"
                    outpath = f"{out_dir}/individual/{env}/{newfilename}"
                    with open(outpath, "wb") as fdst:
                        with open(fasta, "rb") as fsrc:
                            shutil.copyfileobj(fsrc, fdst)
                    with open(fasta, "r") as fsrc:
                        lines = fsrc.readlines()
                    for line in lines:
                        all_genes_soil += line
with open(f"{out_dir}/human_gut_virus_genes_combined.ffn", "w") as out_all_gut:
    out_all_gut.write(all_genes_gut)
with open(f"{out_dir}/freshwater_virus_genes_combined.ffn", "w") as out_all_fw:
    out_all_fw.write(all_genes_fw)
with open(f"{out_dir}/marine_virus_genes_combined.ffn", "w") as out_all_mar:
    out_all_mar.write(all_genes_mar)
with open(f"{out_dir}/soil_virus_genes_combined.ffn", "w") as out_all_soil:
    out_all_soil.write(all_genes_soil)
```

## Dereplicate and cluster viral genes with mmseqs2

Requires mmseqs2 v14.7e284

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
mmseqs easy-search virus_genes/marine_virus_genes_combined.ffn \
virus_genes/marine_virus_genes_combined.ffn \
virus_genes/marine_virus_genes.mmseqs.tsv \
virus_genes/easy-search_tmp \
--threads 90 -s 7.5 --min-seq-id 0.95 -c 0.80 --cov-mode 1 --search-type 3 --format-mode 4 \
--format-output query,target,evalue,gapopen,pident,fident,nident,qstart,qend,qlen,tstart,tend,tlen,alnlen,bits,mismatch
```

## Make a three-column table from the outputs

``` python3
import pandas as pd

dflist = ["virus_genes/human_gut_virus_genes.mmseqs.tsv",
          "virus_genes/freshwater_virus_genes.mmseqs.tsv",
          "virus_genes/marine_virus_genes.mmseqs.tsv",
          "virus_genes/soil_virus_genes.mmseqs.tsv"
          ]

for df in dflist:
    gene_mmseqs_df = pd.read_table(df, index_col=None)
    gene_mmseqs_df = gene_mmseqs_df[["query","target","pident"]]
    gene_mmseqs_df = gene_mmseqs_df.drop_duplicates(ignore_index=True) # Remove duplicate rows that result from clustering database against itself
    gene_mmseqs_df = gene_mmseqs_df.query("query != target") # Remove rows with matches to the same exact sequence that result from clustering database against itself
    gene_mmseqs_df = gene_mmseqs_df.drop_duplicates(subset=["query", "target"], keep='first') # Remove rows with duplicate pairs in reverse (i.e. if there is query: A and target : B above remove rows where query : B and target : A)
    outname = df.replace("virus_genes.mmseqs.tsv", "virus_genes_minid95_mincov80_comparisons.tsv")
    gene_mmseqs_df.to_csv(outname, sep="\t", index=False, header=False)
```

## Produce a clustered graph from the processes mmseqs output using mcl

Requires mcxload v14-137

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
mcxload -abc virus_genes/marine_virus_genes_minid95_mincov80_comparisons.tsv \
-o virus_genes/marine_genesA.mci \
-write-tab virus_genes/marine_genesB.mcltab

# Write to file with mcl
mcl virus_genes/marine_genesA.mci \
-use-tab virus_genes/marine_genesB.mcltab \
-o virus_genes/marine_genes_out.clusters
```

## Pick the “best” genes to represent clusters

“Best” here will just be the longest gene in each cluster, similar to
how the best viral genome was chosen by dRep, above.

``` python3
import sys
import pandas as pd
from Bio import SeqIO
import json
from multiprocessing import Pool, cpu_count
import math

def get_lengths(fasta, out):
    parent = out.split("/")[:-1]
    parent = "/".join(parent)
    out_table = parent + "/gene_lengths.csv"
    rows = []
    with open(fasta, 'r') as FastaFile:
        for rec in SeqIO.parse(FastaFile, 'fasta'):
            seqLen = len(rec)
            name = rec.id
            rows.append(pd.DataFrame({"gene" : [name], "length" : [seqLen]}))
    df = pd.concat(rows, ignore_index=True)
    df.to_csv(out_table, index=False)
    return df

def parse_clusters(chunk_all_lengths):
    chunk, all_lengths_dict = chunk_all_lengths
    start_line, end_line = chunk
    winners = dict()
    members = dict()
    with open(clusterf) as f:
        for line in f.readlines()[start_line:end_line]:
            line = line.strip() 
            genes = line.split("\t")
            clust_name = f"clust_{genes[0]}"
            lengths_dict = {}
            for gene in genes:
                length = all_lengths_dict[gene]
                lengths_dict[gene] = length
            largest = max(lengths_dict, key=lengths_dict.get)
            winners[clust_name] = largest
            members[largest] = genes
    return winners, members

def out_list(win_dict):
    out_string = ""
    for key in win_dict.keys():
        winner = win_dict[key]
        out_string += f"{winner}\n"
    return out_string

clusterf = sys.argv[1]
gene_fasta = sys.argv[2]
out = sys.argv[3]
parent = out.split("/")[:-1]
parent = "/".join(parent)
out_members = parent + "/gene_members.json"
processes = min(cpu_count(), 30)

chunks = []
with open(clusterf) as f:
    lines = f.readlines()
    chunk_size = int(math.ceil(len(lines) / processes))
    for i in range(processes):
        start_line = i * chunk_size
        end_line = min((i + 1) * chunk_size, len(lines))
        chunks.append((start_line, end_line))

df = get_lengths(gene_fasta, out)
all_lengths_dict = df.set_index('gene')['length'].to_dict()

with Pool(processes=processes) as pool:
        combined_args = [(chunk, all_lengths_dict) for chunk in chunks]
        results = pool.map(parse_clusters, combined_args)
winners = {}
members = {}
for result in results:
    result_winners, result_members = result
    winners.update(result_winners)
    members.update(result_members)

outfile = out_list(winners)
with open(out, "w") as f:
    f.write(outfile)
with open(out_members, "w") as out_json:
    json.dump(members, out_json, indent=4)
```

### Execute the script above for each environment

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
python3 choose_genes.py virus_genes/marine_genes_out.clusters virus_genes/marine_virus_genes_combined.ffn virus_genes/marine_best_genes.txt
```

## Build a Bowtie2 mapping index of the best genes in each environment

Requires Bowtie2 v2.5.1.

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
bowtie2-build virus_genes/marine_best_genes.fna virus_genes/bowtie2_index_genes/marine_bowtie2_index_genes --threads 90
```

## Convert sam to sorted bam

Using SAMtools v1.17

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
ls virus_genes/bowtie2_out/marine/*.sam | parallel --jobs 42 'samtools view -bS {} | samtools sort -o /storage2/scratch/kosmopoulos/virome_vs_metagenome/virus_genes/bowtie2_out/marine/{.}.bam -' &
```

## Index sorted bam files

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
ls virus_genes/bowtie2_out/marine/*.bam | parallel --jobs 42 'samtools index {}' &
```

## Filter reads mapping \>= 90% identity with CoverM

Requires CoverM v0.6.1

``` bash
# Execute in mapfile dir for each environment
for BAM in *.bam; do NAME=${BAM%.*}
  coverm filter -b $BAM -o $NAME.filtered.bam --min-read-percent-identity 90 --threads 10
done

ls *.filtered.bam | parallel --jobs 42 'samtools index {}' & # indexes filtered bam files, too
```

## Generate mapped read count tables with CoverM

Requires CoverM v0.6.1.

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
for BAM in virus_genes/bowtie2_out/marine/*.filtered.bam; do
  BASE=$(basename $BAM)
  NAME=${BASE%%.*}
  coverm contig -b $BAM -m count --min-covered-fraction 0 -o virus_genes/bowtie2_out/marine/$NAME.counts -t 30
done
```

## Combine genome breadth tables into matrices for each environment

``` python3
parent = "virus_genes/bowtie2_out/"
envs = ["human_gut", "freshwater", "marine", "soil"]
outparent = "Tables/"

for env in envs:
    path = parent + env
    df_counts_list = []
    df_means_list = []
    df_fractions_list = []
    for file in glob.glob(path + "/*.counts"):
        df = pd.read_csv(file, sep = "\t", index_col=0)
        df.index.name = None
        df.columns = [df.columns[0].split(".")[0]]
        df_counts_list.append(df)
    df_counts = pd.concat(df_counts_list, axis = 1)
    df_counts.to_csv(f"{outparent}gene_counts_{env}.tsv", sep = "\t")
```

## Predict gene functions with Pharokka

Requires Pharokka v1.4.1

``` bash
# One example given for one environment
# Replace "marine" with the correct environment when running for the others
pharokka.py -i virus_genes/marine_virus_genes_combined.ffn -o pharokka_out/marine/ -d pharokka_db -t 100 -f -g prodigal -m
```
